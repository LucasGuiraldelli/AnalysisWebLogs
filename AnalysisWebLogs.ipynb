{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6bc94d-ee74-4e53-a972-e8ff7064840c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### import libs\n",
    "\n",
    "from pyspark.sql.functions import when, split, col, lit, regexp_extract\n",
    "from pyspark.sql.types import LongType, StringType, IntegerType\n",
    "from pyspark.sql import functions as F, Window, DataFrame\n",
    "from typing import List\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4a0369-d086-4a94-8f69-e0048a76c29c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Session's config\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"100\") \n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b6f1ac-9bfa-495c-91c7-6aff9c49bc28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Define Functions\n",
    "\n",
    "class DataValidator:\n",
    "    def __init__(self, df: DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def validate_required_columns(self, required_columns: List[str]):\n",
    "        \"\"\"\n",
    "            Verify if all columns needed really exists\n",
    "\n",
    "            parameters\n",
    "            ---------------\n",
    "            required_columns: list of necessary columns\n",
    "        \n",
    "        \"\"\"\n",
    "        missing_columns = [col for col in required_columns if col not in self.df.columns]\n",
    "        if missing_columns: \n",
    "            raise ValueError(f\"The columns following columns are missing from the dataframe: {', '.join(missing_columns)}\")\n",
    "\n",
    "    def validate_column_types(self, expected_types: dict):\n",
    "        \"\"\"\n",
    "            Verify if each column is in the correct datatype/expected datatype\n",
    "\n",
    "            parameters\n",
    "            ---------------\n",
    "            expected_types: dictionary of column with its datatypes\n",
    "        \"\"\"\n",
    "        for column, expected_type in expected_types.items():\n",
    "            actual_type = self.df.schema[column].dataType.simpleString()\n",
    "            if actual_type != expected_type:\n",
    "                raise ValueError(f\"The column '{column}' should be of type {expected_type}, but it's of type {actual_type}\")\n",
    "\n",
    "    def validate_nulls_in_column(self, column: str):\n",
    "        \"\"\"\n",
    "            Verify if it has null values in a specified column\n",
    "\n",
    "            parameters\n",
    "            ---------------\n",
    "            column: column to check\n",
    "        \"\"\"\n",
    "        if self.df.filter(F.col(column).isNull()).count() > 0:\n",
    "            raise ValueError(f\"There is null values in column: {column}\")\n",
    "\n",
    "    def validate_ip_format(self):\n",
    "        \"\"\"\n",
    "            Validates the ip format, in 'ip' column\n",
    "        \"\"\"\n",
    "        invalid_ips = self.df.filter(~F.expr(\n",
    "            \"ip rlike '^(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\.\"\n",
    "            \"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\.\"\n",
    "            \"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\.\"\n",
    "            \"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$'\"\n",
    "        )).count()\n",
    "        if invalid_ips > 0:\n",
    "            raise ValueError(f\"There is invalid ips in the 'ip' column: {invalid_ips}\")\n",
    "\n",
    "    def validate_status_range(self):\n",
    "        \"\"\"\n",
    "            Verify if 'status' is in the gap between 100 and 599, or a valid status response\n",
    "        \"\"\"\n",
    "        if self.df.filter((F.col(\"status\") < 100) | (F.col(\"status\") > 599)).count() > 0:\n",
    "            raise ValueError(\"There is 'status' of response off the correct gap (100-599)\")\n",
    "\n",
    "    def validate_incompleted_avg(self):\n",
    "        \"\"\"\n",
    "            Verify the average of incomplete lines\n",
    "        \"\"\"\n",
    "        incompleted_avg = self.df.agg(F.avg(\"incompleted\")).collect()[0][0]\n",
    "        if incompleted_avg > 0.2:\n",
    "            raise ValueError(\"Data quality problem... the average of incompleted rows is bigger than 20%\")\n",
    "\n",
    "    def validate_date_format(self):\n",
    "        \"\"\"\n",
    "            Validate the format of the column 'date'\n",
    "        \"\"\"\n",
    "        null_dates = self.df.filter(F.col(\"date\").isNull())\n",
    "        if null_dates.count() > 0:\n",
    "            if null_dates.filter(F.col(\"incompleted\") == 0).count() > 0:\n",
    "                raise ValueError(\"There is null record in 'date' column, verify this\")\n",
    "            logging.warning(\"There is null records in 'date' column, but only in incompleted rows. Check this!\")\n",
    "\n",
    "    def validate_and_save(self, layer: str, path: str):\n",
    "        \"\"\"\n",
    "            Main function of tests, execute tests by determined layer\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            \n",
    "            # Defining rules based on the layer\n",
    "            if layer == 'silver':\n",
    "                # Mandatory columns\n",
    "                required_columns = ['ip', 'date', 'request', 'status', 'size']\n",
    "                self.validate_required_columns(required_columns)\n",
    "\n",
    "                self.validate_incompleted_avg()\n",
    "                self.validate_nulls_in_column(\"ip\")\n",
    "                self.validate_ip_format()\n",
    "                self.validate_status_range()\n",
    "                self.validate_date_format()\n",
    "\n",
    "            elif layer == 'gold':\n",
    "                # Mandatory columns\n",
    "                required_columns = ['ip', 'date', 'request', 'status', 'size', 'endpoint', 'day_of_week']\n",
    "                self.validate_required_columns(required_columns)\n",
    "\n",
    "                expected_types = {\n",
    "                    \"ip\": \"string\",\n",
    "                    \"request\": \"string\",\n",
    "                    \"endpoint\": \"string\",\n",
    "                    \"day_of_week\": \"string\",\n",
    "                    \"status\": \"int\",\n",
    "                    \"size\": \"int\",\n",
    "                    \"date\": \"timestamp\"\n",
    "                }\n",
    "                self.validate_column_types(expected_types)\n",
    "\n",
    "            print(\"Validation succeeded\")\n",
    "            print(f\"Saving data in {layer} layer...\")\n",
    "            self.df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "            print(\"Data saved with success\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6479f818-ed16-4855-a9c9-cf60788d6799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## General Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7980aa0-b32d-498b-9217-82473716930e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from default.access_log_10_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0de97e5-ba0b-4183-b07a-f3f0deb86368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------------------+--------------------+------+-----+\n|            ip|extra_field|               date|             request|status| size|\n+--------------+-----------+-------------------+--------------------+------+-----+\n|10.223.157.186|          -|2009-07-15 21:58:59|      GET / HTTP/1.1|   403|  202|\n|10.223.157.186|          -|2009-07-15 21:58:59|GET /favicon.ico ...|   404|  209|\n|10.223.157.186|          -|2009-07-15 22:50:35|      GET / HTTP/1.1|   200| 9157|\n|10.223.157.186|          -|2009-07-15 22:50:35|GET /assets/js/lo...|   200|10469|\n|10.223.157.186|          -|2009-07-15 22:50:35|GET /assets/css/r...|   200| 1014|\n+--------------+-----------+-------------------+--------------------+------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Regex used to identify data\n",
    "regex_pattern = r'(\\S+) - (.+?)(?: \\[(.*?)\\] \"(.*?)\"(?: (\\d{3}) (\\d+|-)?)?)?$'\n",
    "\n",
    "\n",
    "# Through regex extracting columns from the original data, following the Web Server Access Log pattern\n",
    "df_extracted = df.select('_c0',\n",
    "    F.regexp_extract('_c0', regex_pattern, 1).alias('ip'),\n",
    "    F.regexp_extract('_c0', regex_pattern, 2).alias('extra_field'),\n",
    "    F.regexp_extract('_c0', regex_pattern, 3).alias('date'),\n",
    "    F.regexp_extract('_c0', regex_pattern, 4).alias('request'),\n",
    "    F.regexp_extract('_c0', regex_pattern, 5).alias('status'),\n",
    "    F.regexp_extract('_c0', regex_pattern, 6).alias('size')\n",
    ")\n",
    "\n",
    "# Through regex extracting date and request from 'extra_field' column, as a second step of data gathering\n",
    "## in case of data extracting from those columns, after it, setting '-' into 'extra_field' field\n",
    "df_extracted = df_extracted \\\n",
    "    .withColumn(\"date\", when((col(\"date\") == \"\") & col(\"extra_field\").rlike(r'\\['), split(split(col(\"extra_field\"), r'\\[', 2)[1], r'\\]', 2)[0]).otherwise(col(\"date\"))) \\\n",
    "    .withColumn(\"request\", when((col(\"request\") == \"\") & col(\"extra_field\").rlike(r'\"'), split(split(col(\"extra_field\"), r'\"', 2)[1], r'\"', 2)[0]).otherwise(col(\"request\"))) \\\n",
    "    .withColumn(\"extra_field\", when((col(\"date\") != \"\") | (col(\"request\") != \"\"), \"-\").otherwise(col(\"extra_field\"))) \n",
    "\n",
    "\n",
    "# Datatype correction\n",
    "df_extracted_type_correction = df_extracted.select(\n",
    "    F.col(\"ip\").cast(StringType()).alias(\"ip\"),\n",
    "    F.col(\"extra_field\").cast(StringType()).alias(\"extra_field\"),\n",
    "    F.to_timestamp(F.unix_timestamp(\"date\", \"dd/MMM/yyyy:HH:mm:ss Z\").cast(\"timestamp\")).alias(\"date\"),\n",
    "    F.col(\"request\").cast(StringType()).alias(\"request\"),\n",
    "    F.col(\"status\").cast(\"int\").alias(\"status\"),\n",
    "    F.col(\"size\").cast(IntegerType()).alias(\"size\")\n",
    ")\n",
    "\n",
    "df_extracted_type_correction.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07fb8534-f1b1-4c5d-afa0-f9a54ee9b949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[74]: DataFrame[ip: string, extra_field: string, date: timestamp, request: string, status: int, size: int, incompleted: int]"
     ]
    }
   ],
   "source": [
    "# Creating a flag to indetify 'incompleted' rows of data (eg.: it has id but not date...)\n",
    "df_extracted_type_correction = df_extracted_type_correction.withColumn(\n",
    "    \"incompleted\",\n",
    "    F.when(\n",
    "        (col(\"date\").isNull() | (col(\"date\") == \"\")) &\n",
    "        (col(\"request\").isNull() | (col(\"request\") == \"\")) &\n",
    "        (col(\"status\").isNull() | (col(\"status\") == \"\")) &\n",
    "        (col(\"size\").isNull() | (col(\"size\") == \"\")),\n",
    "        1\n",
    "    ).otherwise(0).cast(IntegerType())\n",
    ")\n",
    "df_extracted_type_correction.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b4da9c-d754-45b0-9546-5ebde1a4640c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is null records in 'date' column, but only in incompleted rows. Check this!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation succeeded\nSaving data in silver layer...\nData saved with success\n"
     ]
    }
   ],
   "source": [
    "# Testing Dataframe and saving in Silver Layer\n",
    "df_silver = DataValidator(df_extracted_type_correction)\n",
    "df_silver.validate_and_save(\"silver\", \"/mnt/delta/layers/silver/logs_delta_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5374cf51-30eb-4690-8930-f4fe61de229d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Feature preparation:\n",
    "df_delta_read = spark.read.format(\"delta\").load(\"/mnt/delta/layers/silver/logs_delta_1\")\n",
    "df_delta_read = df_delta_read.select('ip', 'date', 'request', 'status', 'size')\n",
    "df_delta_read.cache()\n",
    "\n",
    "\n",
    "# (question 2)\n",
    "# Through regex identifying into different columns features 'request_type' and 'endpoint' from orignal 'request' column\n",
    "df = df_delta_read.withColumn(\"request_type\", F.regexp_extract(\"request\", r\"^\\S+\", 0)) \\\n",
    "        .withColumn(\"endpoint\", F.regexp_extract(\"request\", r\"(?<=^\\S+\\s).+(?=\\sHTTP/\\d+\\.\\d+)\", 0))\n",
    "\n",
    "\n",
    "# (question 6)\n",
    "# Identifying 'day_of_week' registered in the logs in - text format (monday -> sunday)\n",
    "df = df.withColumn(\"day_of_week\", F.date_format(\"date\", \"EEEE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763608ed-5228-4e05-a54c-e17f8083decb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation succeeded\nSaving data in gold layer...\nData saved with success\n"
     ]
    }
   ],
   "source": [
    "# Testing Dataframe and saving in Gold Layer\n",
    "df_gold = DataValidator(df)\n",
    "df_gold.validate_and_save(\"gold\", \"/mnt/delta/layers/gold/logs_delta_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5cbc4a8-fb68-4843-a834-ddd0286ca050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093ffdc4-4f9a-488c-b985-7a2f4aa099f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[81]: DataFrame[ip: string, date: timestamp, request: string, status: int, size: int, request_type: string, endpoint: string, day_of_week: string]"
     ]
    }
   ],
   "source": [
    "# Preparation to common dataframe query\n",
    "df_delta_read = spark.read.format(\"delta\").load(\"/mnt/delta/layers/gold/logs_delta_1\")\n",
    "df_delta_read.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f375107-f50c-474a-8d1b-7ee18af071d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1\n",
    "\n",
    "Identifique as 10 maiores origens de acesso (Client IP) por quantidade de acessos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1d3320-5faf-4e36-b8ee-39fcc21289c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n|            ip| count|\n+--------------+------+\n|10.216.113.172|158614|\n|  10.220.112.1| 51942|\n|10.173.141.213| 47503|\n|10.240.144.183| 43592|\n|  10.41.69.177| 37554|\n|10.169.128.121| 22516|\n| 10.211.47.159| 20866|\n| 10.96.173.111| 19667|\n| 10.203.77.198| 18878|\n|   10.31.77.18| 18721|\n+--------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "top10 = df_delta_read.groupBy('ip').count().sort(F.desc(\"count\")).limit(10)\n",
    "top10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2ca0f8-6569-4a34-9bbd-9fd8730de696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2\n",
    "\n",
    "Liste os 6 endpoints mais acessados, desconsiderando aqueles que representam arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8878841-4ffd-4883-b66a-7fcc62a10f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----+\n|endpoint                              |count|\n+--------------------------------------+-----+\n|/                                     |99303|\n|/release-schedule/                    |25937|\n|/search/                              |23055|\n|/release-schedule                     |18940|\n|/release-schedule/?p=1&r=&l=&o=&rpp=10|8415 |\n|/news/                                |7505 |\n+--------------------------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter data that 'endpoint' don't have a file extension\n",
    "df_no_file_endpoints = df_delta_read.filter(~F.col(\"endpoint\").rlike(r\"\\.\\w+$\"))\n",
    "\n",
    "window = Window.orderBy(F.desc(\"count\"))\n",
    "top6 = (df_no_file_endpoints\n",
    "        .groupBy(\"endpoint\")\n",
    "        .count()\n",
    "        .withColumn(\"rank\", F.row_number().over(window))\n",
    "        .filter(F.col(\"rank\") <= 6)\n",
    "        .drop(\"rank\"))\n",
    "\n",
    "top6.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5aa1fcf-c015-4f40-8f1a-1103684a084b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3\n",
    "\n",
    "Qual a quantidade de Client IPs distintos?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72337352-8bd7-4136-b906-be152a6cd719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Client IPs distintos: 333923\n"
     ]
    }
   ],
   "source": [
    "# Getting only the distinct ip's and counting\n",
    "uniqueIDS = df_delta_read.select(\"ip\").distinct().count()\n",
    "print(f\"Quantidade de Client IPs distintos: {uniqueIDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6566a4b0-626c-43d1-a64f-97567e274665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4\n",
    "\n",
    "Quantos dias de dados estão representados no arquivo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26df2c7c-f209-4f27-a3c8-224fa2a37ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value: 2009-07-15 21:58:59\nMaximum value: 2011-12-03 21:28:11\nDays between: 870\n"
     ]
    }
   ],
   "source": [
    "# It collects the min and max date founded in data\n",
    "min_date = df_delta_read.agg(F.min(\"date\")).collect()[0][0]\n",
    "max_date = df_delta_read.agg(F.max(\"date\")).collect()[0][0]\n",
    "\n",
    "# Using the dates above, calculate the difference in days\n",
    "days_difference = (max_date - min_date).days\n",
    "\n",
    "print(f\"Minimum value: {min_date}\")\n",
    "print(f\"Maximum value: {max_date}\")\n",
    "print(f\"Days between: {days_difference}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9471b6d3-a196-48ca-9b14-57aa3b68bfc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days of existant data: 792\n"
     ]
    }
   ],
   "source": [
    "# ambiguos question\n",
    "\n",
    "# Extract only year month and day and caculating only the dates that has data\n",
    "unique_days = df_delta_read.select(F.to_date(\"date\", \"yyyy-MM-dd HH:mm:ss\").alias(\"day_only\")).distinct().count()\n",
    "print(f\"Days of existant data: {unique_days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70e1501-66e1-41b2-adcd-79f4602a6fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5\n",
    "\n",
    "Com base no tamanho (em bytes) do conteúdo das respostas, faça a seguinte análise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b439b798-3a0b-4a6b-b5e2-abbd085c2908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of response size: 767916.81 MB\nBiggest response in size: 76.50 MB\nMinor response in size: 0.00 MB\nAverage of response size: 0.19 MB\n"
     ]
    }
   ],
   "source": [
    "# Executing data agg\n",
    "stats = df_delta_read.agg(\n",
    "    F.sum(\"size\").alias(\"total_bytes\"), # get sum 5.1\n",
    "    F.max(\"size\").alias(\"max_bytes\"),   # get max 5.2\n",
    "    F.min(\"size\").alias(\"min_bytes\"),   # get min 5.3\n",
    "    F.avg(\"size\").alias(\"avg_bytes\")    # get avg 5.4\n",
    ").collect()[0]\n",
    "\n",
    "# converting data to MegaBytes to make easy the reading\n",
    "total_gb = stats[\"total_bytes\"] / (1024 ** 2)\n",
    "max_gb = stats[\"max_bytes\"] / (1024 ** 2)\n",
    "min_gb = stats[\"min_bytes\"] / (1024 ** 2)\n",
    "avg_gb = stats[\"avg_bytes\"] / (1024 ** 2)\n",
    "\n",
    "print(f\"Sum of response size: {total_gb:.2f} MB\")\n",
    "print(f\"Biggest response in size: {max_gb:.2f} MB\")\n",
    "print(f\"Minor response in size: {min_gb:.2f} MB\")\n",
    "print(f\"Average of response size: {avg_gb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab4a9ba-be51-4709-928c-d971fe43b40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6\n",
    "\n",
    "Qual o dia da semana com o maior número de erros do tipo \"HTTP Client Error\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c0d780-2fd5-4938-9c02-6f86067978a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n|day_of_week|error_count|\n+-----------+-----------+\n|Friday     |15087      |\n+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Calculates errors grouped by 'day_of_week'\n",
    "## setting the patthern to identify which code means HTTP Client Error\n",
    "\n",
    "common_error_day = df_delta_read.filter((F.col(\"status\") > 399) & (F.col(\"status\") < 500)) \\\n",
    "        .groupBy(\"day_of_week\") \\\n",
    "        .agg(F.count(\"*\").alias(\"error_count\")) \\\n",
    "        .orderBy(F.desc(\"error_count\")) \\\n",
    "        .limit(1)\n",
    "\n",
    "common_error_day.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AnalysisWebLogs",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
